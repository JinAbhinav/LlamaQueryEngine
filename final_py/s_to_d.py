import subprocess
import pandas as pd

# Function to query Ollama model locally
def query_ollama(prompt, model="llama3.2"):
    try:
        # Run Ollama model using subprocess
        result = subprocess.run(
            ['ollama', 'run', model],   # Running the model locally
            input=prompt,              # Sending the prompt as input
            capture_output=True,       # Capture the output of the command
            text=True                  # Get the output as a string
        )
        
        # Check if the subprocess ran successfully
        if result.returncode != 0:
            return f"Error: {result.stderr}"
        
        # Extract the content from the output
        content = result.stdout.strip()
        
        # If no content is returned, handle accordingly
        if not content:
            return "No content generated by the model. Please check the model's status."
        
        return content

    except Exception as e:
        return f"Error: {str(e)}"


# Sample DataFrame (This will hold the data to query)
data = {
    "id": [1, 2, 3],
    "name": ["Alice", "Bob", "Charlie"],
    "age": [25, 30, 35],
    "department": ["HR", "IT", "Finance"]
}
df = pd.DataFrame(data)

# Function to query the DataFrame using Ollama
def query_dataframe(query, dataframe):
    # Convert the DataFrame to a string for the LLM to process
    dataframe_text = dataframe.to_string(index=False)
    
    # Formulate the prompt
    prompt = f"""
    You are a helpful assistant. Below is the data from a company:

    {dataframe_text}

    Please answer the following query:
    {query}
    """
    
    # Query Ollama API and return the response
    return query_ollama(prompt)

# Example: User query
user_query = "who is the youngest?"

# Get Ollama response
response = query_dataframe(user_query, df)
    
# Display the response
print(response)